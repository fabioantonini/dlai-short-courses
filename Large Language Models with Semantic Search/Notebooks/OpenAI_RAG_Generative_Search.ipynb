{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe1ceeb",
   "metadata": {},
   "source": [
    "# Generative Search (RAG) with OpenAI + Local Vector DB (Chroma)\n",
    "\n",
    "This notebook mirrors the lesson flow:\n",
    "\n",
    "1. Build a text archive\n",
    "2. Chunk it\n",
    "3. Create embeddings\n",
    "4. Store & search with a lightweight local vector DB\n",
    "5. Add a generation step to answer questions using retrieved context\n",
    "\n",
    "> Requirements: an environment variable `OPENAI_API_KEY` must be set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025a7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (Optional) If you run this notebook locally, you may need:\n",
    "# !pip -q install -U openai chromadb python-dotenv tiktoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecaf319",
   "metadata": {},
   "source": [
    "## 1) Define a question and build a small text archive\n",
    "\n",
    "Replace `text` with your own documents (articles, chapters, PDFs converted to text, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbef0c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Are side projects important when you are starting to learn about AI?\"\n",
    "\n",
    "# Example mini-archive (replace with your own corpus)\n",
    "text = \"\"\"\n",
    "The rapid rise of AI has led to a rapid rise in AI jobs, and many people are building exciting careers in this field. A career is a decades-long journey, and the path is not always straight. In the early stages, focus on building skills, shipping small projects, and learning to learn.\n",
    "\n",
    "Side projects can be a powerful way to explore ideas and practice new skills. Even if you have a full-time job, a side hustle or a fun project can stir the creative juices and help you grow.\n",
    "\n",
    "As you build your portfolio, be mindful of your employer's policies. Avoid conflicts of interest and respect confidentiality.\n",
    "\"\"\"\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecadd3",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "\n",
    "Load the API key and initialize the OpenAI client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38694d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # reads .env if present\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY. Set it as an environment variable or in a .env file.\")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c80e4b",
   "metadata": {},
   "source": [
    "## 3) Chunking\n",
    "\n",
    "Split the archive into chunks that each contain one coherent idea.\n",
    "\n",
    "Tip: For real corpora, consider chunk sizes of ~200â€“800 tokens with overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfe4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Simple paragraph chunking (works well for clean text)\n",
    "chunks = [c.strip() for c in text.split(\"\\n\\n\") if c.strip()]\n",
    "\n",
    "# Optional: remove excessive whitespace\n",
    "chunks = [re.sub(r\"\\s+\", \" \", c).strip() for c in chunks]\n",
    "\n",
    "print(\"Num chunks:\", len(chunks))\n",
    "chunks[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda2b9f",
   "metadata": {},
   "source": [
    "## 4) Embeddings (OpenAI)\n",
    "\n",
    "We embed each chunk so that semantic search can work by similarity in vector space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c7db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"  # lightweight + solid default\n",
    "\n",
    "def embed_texts(texts: List[str], batch_size: int = 64) -> List[List[float]]:\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        vectors.extend([d.embedding for d in resp.data])\n",
    "    return vectors\n",
    "\n",
    "chunk_embeddings = embed_texts(chunks)\n",
    "len(chunk_embeddings), len(chunk_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46188706",
   "metadata": {},
   "source": [
    "## 5) Store vectors in a lightweight local Vector DB (Chroma)\n",
    "\n",
    "Chroma is easy to run locally (no server required) and supports metadata + filtering.\n",
    "\n",
    "We will store:\n",
    "- `documents`: the text chunks\n",
    "- `embeddings`: their vectors\n",
    "- `metadatas`: optional info (source, paragraph id, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e191a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Use persistent storage on disk (change the path if you want)\n",
    "CHROMA_PATH = \"./chroma_rag_db\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "COLLECTION_NAME = \"lesson5_rag\"\n",
    "# Recreate collection to keep this notebook idempotent\n",
    "try:\n",
    "    chroma_client.delete_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(name=COLLECTION_NAME, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "ids = [f\"chunk-{i}\" for i in range(len(chunks))]\n",
    "metadatas = [{\"chunk_id\": i} for i in range(len(chunks))]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks,\n",
    "    embeddings=chunk_embeddings,\n",
    "    metadatas=metadatas\n",
    ")\n",
    "\n",
    "collection.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08222370",
   "metadata": {},
   "source": [
    "## 6) Semantic search (Dense Retrieval)\n",
    "\n",
    "We embed the query and retrieve the top-$k$ most similar chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa26d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 3):\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    res = collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    # Flatten one-query result\n",
    "    docs = res[\"documents\"][0]\n",
    "    metas = res[\"metadatas\"][0]\n",
    "    dists = res[\"distances\"][0]\n",
    "    return list(zip(docs, metas, dists))\n",
    "\n",
    "top = retrieve(question, k=3)\n",
    "for i,(doc, meta, dist) in enumerate(top, 1):\n",
    "    print(f\"#{i} | chunk_id={meta['chunk_id']} | distance={dist:.4f}\")\n",
    "    print(doc[:200] + (\"...\" if len(doc) > 200 else \"\"))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115bb5a",
   "metadata": {},
   "source": [
    "## 7) Generation step (RAG)\n",
    "\n",
    "Now we **inject retrieved context** into the prompt and ask an LLM to answer.\n",
    "\n",
    "Design choices you can tune:\n",
    "- number of retrieved chunks ($k$)\n",
    "- prompt format (instructions, citations, style)\n",
    "- model and max output tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debb2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_MODEL = \"gpt-4.1-mini\"  # choose any model available in your account\n",
    "\n",
    "def answer_with_rag(question: str, k: int = 3, max_output_tokens: int = 200):\n",
    "    retrieved = retrieve(question, k=k)\n",
    "    context_blocks = []\n",
    "    for rank,(doc, meta, dist) in enumerate(retrieved, 1):\n",
    "        context_blocks.append(f\"[{rank}] (chunk_id={meta['chunk_id']}) {doc}\")\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = f\"\"\"You are answering using ONLY the provided context.\n",
    "If the answer is not contained in the context, say: \"I don't know based on the provided text.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (concise):\n",
    "\"\"\"\n",
    "\n",
    "    # Preferred: Responses API\n",
    "    resp = client.responses.create(\n",
    "        model=LLM_MODEL,\n",
    "        input=prompt,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "    )\n",
    "    return resp.output_text, retrieved\n",
    "\n",
    "final_answer, retrieved = answer_with_rag(question, k=3, max_output_tokens=200)\n",
    "print(final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caede97",
   "metadata": {},
   "source": [
    "## 8) Optional: multiple generations for quick prompt evaluation\n",
    "\n",
    "Generate multiple answers to see variability and test prompt robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade2a11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_with_rag_n(question: str, k: int = 3, n: int = 3, max_output_tokens: int = 200):\n",
    "    retrieved = retrieve(question, k=k)\n",
    "    context = \"\\n\\n\".join([f\"[{i+1}] {doc}\" for i,(doc,meta,dist) in enumerate(retrieved)])\n",
    "\n",
    "    prompt = f\"\"\"You are answering using ONLY the provided context.\n",
    "If the answer is not contained in the context, say: \"I don't know based on the provided text.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (concise):\n",
    "\"\"\"\n",
    "\n",
    "    answers = []\n",
    "    for _ in range(n):\n",
    "        r = client.responses.create(model=LLM_MODEL, input=prompt, max_output_tokens=max_output_tokens)\n",
    "        answers.append(r.output_text)\n",
    "    return answers\n",
    "\n",
    "answers = answer_with_rag_n(question, k=3, n=3, max_output_tokens=200)\n",
    "for i,a in enumerate(answers, 1):\n",
    "    print(f\"--- Generation {i} ---\")\n",
    "    print(a)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ccc73-366b-4591-ac16-0f5041aa6704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
