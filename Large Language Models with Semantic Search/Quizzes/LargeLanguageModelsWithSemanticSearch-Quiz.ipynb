{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8ca60d",
   "metadata": {},
   "source": [
    "# Large Language Models with Semantic Search — Quiz\n",
    "\n",
    "### 1. What is the main limitation of pure keyword (lexical) search when users phrase queries differently from the documents?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Keyword search relies on exact (or near-exact) word overlap, so it can miss relevant documents that use synonyms, paraphrases, or different wording even when the meaning matches.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What does the BM25 algorithm score, at a high level, and why is it efficient?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "BM25 scores a document based on how well its terms match the query terms (with term-frequency and length normalization). It is efficient because it uses an inverted index.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What is an inverted index and what problem does it solve in search systems?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "An inverted index maps each keyword to the list of documents where it appears, enabling fast retrieval without scanning the entire corpus.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What is an embedding in the context of semantic search?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "An embedding is a vector representation of text such that semantic similarity corresponds to geometric closeness in vector space.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Why can two different sentences with similar meaning end up close in embedding space?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Because embedding models capture semantic relationships and map paraphrases to nearby vectors even when they share few exact words.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is dense retrieval (vector search) and what does it return?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Dense retrieval embeds queries and documents and returns the nearest neighbors in embedding space based on semantic similarity.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Why can dense retrieval return results that are similar but not the correct answer?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Because similarity in embedding space does not guarantee factual correctness or direct answer relevance.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 8. What is a reranker and where does it fit in the search pipeline?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "A reranker re-scores retrieved candidates and reorders them by relevance after the initial retrieval stage.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How are rerank models trained at a high level?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "They are trained on labeled (query, document) pairs, rewarding relevant pairs with high scores and penalizing non-relevant pairs.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 10. What is the key benefit of adding reranking on top of keyword search?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "It improves precision by identifying which keyword-matched documents truly answer the query.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What is hybrid search and why is it useful?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Hybrid search combines lexical and semantic scores to leverage both exact matches and meaning-based similarity.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Why are multilingual embedding models useful for semantic search?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "They map semantically equivalent text across languages into a shared vector space.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 13. What is approximate nearest neighbor (ANN) search and why is it used?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "ANN algorithms approximate exact nearest neighbor search to achieve much faster retrieval at large scale.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Give two examples of ANN libraries and what they typically store.\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Examples include FAISS and Annoy. They typically store vectors and ids for efficient similarity search.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 15. What advantages do vector databases provide over simple ANN libraries?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Vector databases store vectors with metadata, support filtering, and allow easier incremental updates.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 16. Why is chunking necessary when building a vector index from raw documents?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Chunking splits long documents into manageable units for embedding and retrieval.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 17. What is a common chunking pitfall and one mitigation strategy?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "Chunks that are too small may lose context; adding overlap or metadata can mitigate this.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 18. What is Retrieval-Augmented Generation (RAG)?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "RAG retrieves relevant passages and injects them into an LLM prompt to generate grounded answers.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 19. How does adding retrieved context typically affect factuality?\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "It increases factual grounding by anchoring answers to retrieved evidence.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### 20. Name three common metrics used to evaluate search systems.\n",
    "\n",
    "<details>\n",
    "<summary><b>Show Answer</b></summary>\n",
    "\n",
    "MAP, MRR, and NDCG — metrics that evaluate ranking quality and relevance ordering.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
