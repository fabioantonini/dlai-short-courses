{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: RAG with PDF Documents\n",
    "\n",
    "Welcome to this assignment! In this exercise, you'll explore how to perform **Retrieval-Augmented Generation (RAG)** with PDF documents. This format was selected since it is very common but you can do this same process for all sorts of documents. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "- Understand how to use **LangChain** for building RAG pipelines.\n",
    "- Learn to manage and query document embeddings using **ChromaDB**.\n",
    "- Build an interactive user interface for querying documents using **Gradio**.\n",
    "\n",
    "Let’s dive in and uncover how RAG can make document retrieval smarter and more efficient!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "- You can add the comment # grade-up-to-here in any graded cell to signal the grader that it must only evaluate up to that point. This is helpful if you want to check if you are on the right track even if you are not done with the whole assignment. Be sure to remember to delete the comment afterwards!\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "- To submit your notebook, save it and then click on the blue submit button at the beginning of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "In this step, we ensure that the required environment variables are properly loaded for the lab.\n",
    "\n",
    "### Key Points:\n",
    "- The `dotenv` library is used to load environment variables from a `.env` file.\n",
    "- The code checks if the `OPENAI_API_KEY` is set, which is required for accessing OpenAI's API.\n",
    "- If the API key is found, a confirmation message is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "\tprint(\"OpenAI API key found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Global Variables\n",
    "\n",
    "This cell sets up important global variables that will be used throughout the lab:\n",
    "\n",
    "- **`PDF_DOCS_PATH`**: Specifies the path to the directory containing the PDF documents.\n",
    "\n",
    "- **`EMBEDDINGS`**: Initializes the embeddings model using OpenAI's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "PDF_DOCS_PATH = \"./docs\"\n",
    "\n",
    "EMBEDDINGS = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing PDF Files\n",
    "\n",
    "This cell retrieves all PDF files from the specified directory:\n",
    "\n",
    "- **`glob.glob`**: Searches for files matching the `.pdf` pattern in the directory defined by `PDF_DOCS_PATH`.\n",
    "\n",
    "- The resulting list, `pdf_files`, contains the paths of all PDF documents found.\n",
    "\n",
    "The output displays the identified PDF files, which will be processed in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "# Get all .pdf files in the base directory and its subdirectories\n",
    "pdf_files = glob.glob(os.path.join(PDF_DOCS_PATH, \"*.pdf\"))\n",
    "\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case you will be working with a gardening manual provided by [High Rocks](https://highrocks.org/). However, once you are done with this assignment you can reuse all of this code for your own documents, notice that the code above will locate all PDF files inside the `./docs/` directory which is super useful if you have more than one file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: `clean_text`\n",
    "\n",
    "This pre-defined function is provided for your convenience and requires no additional modifications. It preprocesses text to ensure consistent formatting by:\n",
    "\n",
    "- Replacing newline characters (`\\n`) with spaces.\n",
    "- Removing extra spaces to create clean, well-structured text.\n",
    "\n",
    "You can directly use this function later in the lab for cleaning text extracted from the PDF documents.\n",
    "\n",
    "Whenever you are dealing with text you will usually encounter a function similar to this one which will help with the preprocessing to yield better results. This one is pretty basic but these kinds of functions can be done in any way required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Replace '\\n' with spaces and remove multiple spaces\n",
    "    cleaned = \" \".join(text.split(\"\\n\"))  # Join on newlines first\n",
    "    cleaned = \" \".join(cleaned.split())  # Remove extra spaces\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the `load_pdf` Function\n",
    "\n",
    "In this exercise, you will write a function to process and clean the content of a PDF document. The function should:\n",
    "\n",
    "1. Load the PDF file from the specified path.\n",
    "2. Extract the content of the document, processing each page individually.\n",
    "3. Use the `clean_text` helper function to ensure the text is properly formatted.\n",
    "\n",
    "### Hints:\n",
    "- Utilize [`PyPDFLoader`](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/) to handle the PDF loading.\n",
    "- Iterate through the documents to apply text cleaning.\n",
    "- Return the cleaned content in a format suitable for further processing.\n",
    "\n",
    "This function will be a foundational part of your pipeline for working with the PDF data. After completing this assignment, if you wish to use this code for your own RAG pipelines you might opt to use a different loader depending on the format of your data. Langchain provides a bunch of loaders which you can check in the [docs](https://python.langchain.com/docs/how_to/#document-loaders) and cover formats such as HTML, JSON and CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path):\n",
    "\n",
    "\t### START CODE HERE ###\n",
    "\n",
    "\t# Use the PyPDFLoader by specifying the correct path\n",
    "\tloader = None\n",
    "\n",
    "\t# Use the load method from the loader to get the documents\n",
    "\tdocuments = None\n",
    "\n",
    "\t# Iterate over the documents\n",
    "\tfor None in None:\n",
    "\t\t# Apply the clean_text function to the page_content attribute of each document\n",
    "\t\tNone.None = None(None.None)\n",
    "\n",
    "\t### END CODE HERE ###\n",
    "\t\n",
    "\treturn documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, the `load_pdf` function will be applied to all the PDF files, combining the cleaned content into a single list. In this case there is a single document so this process is quite fast but it might be slower depending on the amount and size of the files. Notice that the resulting `docs` has a size of 42 which is the same number of pages as the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "docs = [doc for pdf in pdf_files for doc in load_pdf(pdf)]\n",
    "\n",
    "print(f\"There are a total of {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "There are a total of 42 documents\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "# unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement the `split_documents` function\n",
    "\n",
    "In this exercise, you will write a function to split the loaded document content into smaller chunks for easier processing. The function should:\n",
    "\n",
    "1. Use the `RecursiveCharacterTextSplitter` class to split the document content into chunks. You can read more about it [here](https://python.langchain.com/docs/how_to/recursive_text_splitter/#splitting-text-from-languages-without-word-boundaries).\n",
    "2. Adjust the chunk size and overlap to ensure that each chunk is appropriately sized while maintaining context between chunks.\n",
    "3. Return the resulting list of document splits.\n",
    "\n",
    "### Hints:\n",
    "- The `RecursiveCharacterTextSplitter` class allows you to configure how the text is split. You’ll need to adjust the following parameters:\n",
    "\n",
    "  - **`chunk_size`**: Set the maximum size of each chunk (use 1500 characters for this exercise).\n",
    "\n",
    "  - **`chunk_overlap`**: Define the number of characters to overlap between chunks (use 150 characters for this exercise).\n",
    "  - **`separators`**: Specify the separators that should be used to break the text into chunks. These can include sentence-ending punctuation or newlines. These are already provided.\n",
    "  - **`keep_separator`**: Set to `True` to retain the separator in the split text.\n",
    "\n",
    "- To perform the actual splitting you need to use the [`split_documents`](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#langchain_text_splitters.character.RecursiveCharacterTextSplitter.split_documents) method from `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "\n",
    "This function will help break down long documents into manageable pieces that are suitable for embedding and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def split_docs(docs):\n",
    "\n",
    "\t### START CODE HERE ###\n",
    "\n",
    "\t# Instantiate the RecursiveCharacterTextSplitter class with the appropriate parameters\n",
    "\ttext_splitter = RecursiveCharacterTextSplitter( \n",
    "\t\tchunk_size=None,\n",
    "\t\tchunk_overlap=None,\n",
    "\t\tseparators=[\". \", \"? \", \"! \", \"\\n\\n\", \"\\n\", \" \", \"\"], \n",
    "\t\tkeep_separator=None,\n",
    "\t)\n",
    "\n",
    "\t# Use the splitter to split the documents (use the split_documents method)\n",
    "\tsplits = None\n",
    "\n",
    "\t### END CODE HERE ###\n",
    "\n",
    "\treturn splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the splitting function to the actual documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "splitted_docs = split_docs(docs)\n",
    "\n",
    "print(f\"There are a total of {len(splitted_docs)} documents after splitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "There are a total of 52 documents after splitting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "# unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement the `setup_vectordb` function\n",
    "\n",
    "In this exercise, you will write a function to set up a vector database for efficient document retrieval. The function should:\n",
    "\n",
    "1. Use the `Chroma.from_documents` method to create a vector database from the split documents.\n",
    "2. Embed the documents using the specified embeddings model.\n",
    "3. Store the vector database in the specified directory for persistent storage.\n",
    "\n",
    "### Hints:\n",
    "\n",
    "- The method accepts three parameters:\n",
    "\n",
    "  - **`splits`**: The list of document splits to be embedded.\n",
    "  \n",
    "  - **`embeddings`**: The embeddings model used to convert documents into vectors. Here you can use the global variable for embeddings defined earlier.\n",
    "  - **`db_docs_path`**: The directory where the vector database will be stored. The default is `db/chroma/`. Use the provided parameter `db_docs_path` to ensure a proper grading.\n",
    "\n",
    "\n",
    "This function sets up the vector database that will be used for document retrieval in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "def setup_vectordb(splitted_docs, db_docs_path=\"db/chroma/\"):\n",
    "\n",
    "\t# Delete the in-memory directory that will hold the data\n",
    "\t# This is done in case you run this function multiple times to avoid duplicated documents\n",
    "\tif os.path.exists(db_docs_path) and os.path.isdir(db_docs_path):\n",
    "\t\tshutil.rmtree(db_docs_path)\n",
    "\n",
    "\t### START CODE HERE ###\n",
    "\n",
    "\t# Create an instance of the vector database\n",
    "\tvectordb = Chroma.from_documents( \n",
    "\t\tdocuments=None,\n",
    "\t\tembedding=None,\n",
    "\t\tpersist_directory=None,\n",
    "\t)\n",
    "\n",
    "\t### END CODE HERE ###\n",
    "\n",
    "\treturn vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the function to create the vector database that contains the post-splitting documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "DATABASE = setup_vectordb(splitted_docs)\n",
    "\n",
    "if os.path.exists(\"./db/chroma/\"):\n",
    "\tprint(\"Successfully created the vector database!\")\n",
    "else:\n",
    "\tprint(\"The directory to store the vector database was not created, double check your code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Successfully created the vector database!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "# unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try asking the database to retrieve the top k (5 in this case) documents given a question. You might get duplicated results but that is ok!\n",
    "\n",
    "You can try out different questions and values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "question = \"How can I plant tomatoes?\"\n",
    "retrieved_docs = DATABASE.similarity_search(question, k=5)\n",
    "\n",
    "for rd in retrieved_docs:\n",
    "\tprint(rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: `format_docs`\n",
    "\n",
    "To be able to visualize the retrieved documents in a more organized manner, the `format_docs` function is provided. This function prints out some information about the documents such as their number, contents, document of origin and page:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source_path = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        filename = (\n",
    "            os.path.basename(source_path) if source_path != \"Unknown\" else \"Unknown\"\n",
    "        )\n",
    "\n",
    "        result = (\n",
    "            f\"Result {i}:\\n\"\n",
    "            f\"{doc.page_content}\\n\\n\"\n",
    "            f\"Document: {filename}\\n\"\n",
    "            f\"Page: {doc.metadata.get('page', 'Unknown')}\"\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "    return \"\\n---\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out with the retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement the `process_query` function\n",
    "\n",
    "In this exercise, you will complete the logic for handling user queries in a retrieval-augmented generation (RAG) pipeline. Notice that the first two steps have already been provided for you:\n",
    "\n",
    "1. Initialize a language model (LLM) for generating responses.\n",
    "\n",
    "2. Define a custom prompt template to ensure detailed, thorough answers.\n",
    "\n",
    "\n",
    "### Hints:\n",
    "- Use **`PromptTemplate.from_template`** to convert the provided template into a prompt object.\n",
    "- Retrieve the source documents:\n",
    "  - Use the **`as_retriever`** method on `DATABASE` to set up a retriever.\n",
    "  - Invoke the retriever with the given question to get the relevant documents.\n",
    "  - Use the `format_docs` helper function to process and format the retrieved documents.\n",
    "- Set up the QA pipeline:\n",
    "  - Define a chain where:\n",
    "    - **Context** is set to the retriever output.\n",
    "    - **Question** is directly passed through with `RunnablePassthrough`.\n",
    "    - **Prompt** is piped into the QA chain.\n",
    "    - **LLM** processes the prompt.\n",
    "    - **StrOutputParser** is used to parse the LLM's output.\n",
    "  - Use the `invoke` method on the QA chain to generate the LLM's response.\n",
    "\n",
    "This task will test your ability to integrate multiple components in the RAG pipeline effectively. For more info be sure to check the [docs](# https://python.langchain.com/docs/how_to/inspect/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 878
   },
   "outputs": [],
   "source": [
    "def process_query(question):\n",
    "\t\n",
    "\t# Initialize the LLM\n",
    "\tllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\t# Define a template for the QA\n",
    "\ttemplate = \"\"\"Use the following pieces of context to answer the question at the end. Provide a detailed, thorough response that:\n",
    "\t\t1. Answers the main question\n",
    "\t\t2. Provides relevant examples or details from the context\n",
    "\t\t3. Explains any important concepts mentioned\n",
    "\t\t4. If relevant, discusses implications or applications\n",
    "\n",
    "\t\tIf you don't know the answer, provide a detailed explanation of what aspects you're uncertain about and why.\n",
    "\n",
    "\t\t{context}\n",
    "\t\tQuestion: {question}\n",
    "\t\tDetailed Answer:\"\"\"\n",
    "\n",
    "\t### START CODE HERE ###\n",
    "\n",
    "\t# Instantiate a PromptTemplate using the template given\n",
    "\tprompt = None\n",
    "\n",
    "\t# Use the as_retriever method to use the DATABASE as a retriever\n",
    "\tretriever = None\n",
    "\t\n",
    "\t# Get the source documents by using the invoke method on the retriever and passing the question\n",
    "\tsource_documents = None\n",
    "\n",
    "\t# Format the source documents using the format_docs helper function\n",
    "\tdoc_references = None\n",
    "\n",
    "\t# Set up the QA chain\n",
    "\tqa_chain = ( \n",
    "\t# Use the retriever as context and a RunnablePassthrough as question\n",
    "\t{\"context\": None, \"question\": None}\n",
    "\t# Pipe to the prompt\n",
    "\t| None\n",
    "\t# Pipe to the llm\n",
    "\t| None\n",
    "\t# Pipe to the StrOutputParser\n",
    "\t| None\n",
    "\t) \n",
    "\n",
    "\t# Get response from qa_chain by using the invoke method and passing the question\n",
    "\tllm_response = None\n",
    "\n",
    "\t### END CODE HERE ###\n",
    "\n",
    "\treturn llm_response, doc_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "question = \"How can I plant flowers?\"\n",
    "\n",
    "llm_response, doc_references = process_query(question)\n",
    "\n",
    "print(f\"### LLM Response #################\\n\\n{llm_response}\\n\")\n",
    "print(f\"### References ###################\\n\\n{doc_references}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "# unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio User Interface\n",
    "\n",
    "This cell creates an interactive Gradio interface for you to interact with the Q&A Assistant. The interface allows users to ask questions and receive detailed answers along with source document references.\n",
    "\n",
    "### Key Features:\n",
    "- **Input Section**: A textbox for entering questions, with a \"Submit Question\" button for submission.\n",
    "- **Tabs for Output**:\n",
    "  - **AI Response**: Displays the detailed answer generated by the AI.\n",
    "  - **Document References**: Shows the source documents referenced in the response.\n",
    "- **Interactivity**:\n",
    "  - The \"Submit Question\" button triggers the `process_query` function to process the input and display the results.\n",
    "  - Pressing \"Enter\" in the question textbox also submits the question.\n",
    "\n",
    "This user-friendly interface enables interaction with the QA chatbot in a structured and visually organized way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 761
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Monochrome()) as iface:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # 📚 DAG Scripts Q&A Assistant\n",
    "        Ask any question about the DAG C1 & C2 content and get detailed answers with source references.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"### Your Question\")\n",
    "        question_input = gr.Textbox(\n",
    "            lines=3,\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            label=\"\",  # Removed the label since we're using Markdown\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Submit Question\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"📝 Response\"):\n",
    "            gr.Markdown(\"### AI Response\")\n",
    "            response_output = gr.Textbox(\n",
    "                lines=15,\n",
    "                label=\"\",  # Removed the label since we're using Markdown\n",
    "                show_copy_button=True,\n",
    "            )\n",
    "        with gr.TabItem(\"🔍 Document References\"):\n",
    "            gr.Markdown(\"### Source Documents\")\n",
    "            references_output = gr.Textbox(\n",
    "                lines=15,\n",
    "                label=\"\",  # Removed the label since we're using Markdown\n",
    "                show_copy_button=True,\n",
    "            )\n",
    "\n",
    "    # Add submit button click event and enter key functionality\n",
    "    submit_btn.click(\n",
    "        fn=process_query,\n",
    "        inputs=[question_input],\n",
    "        outputs=[response_output, references_output],\n",
    "    )\n",
    "    question_input.submit(\n",
    "        fn=process_query,\n",
    "        inputs=[question_input],\n",
    "        outputs=[response_output, references_output],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Close the server (in case you run this cell multiple times)\n",
    "iface.close()\n",
    "\n",
    "# Spin up the gradio app\n",
    "iface.launch(server_name=\"0.0.0.0\", share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You’ve successfully completed the assignment! \n",
    "\n",
    "In this lab, you:\n",
    "- Processed and cleaned PDF documents for use in a retrieval-augmented generation (RAG) pipeline.\n",
    "- Split documents into manageable chunks for embedding.\n",
    "- Set up a vector database for efficient retrieval.\n",
    "- Implemented a QA pipeline to handle user queries with detailed responses and source references.\n",
    "- Built an interactive Gradio interface to bring the system to life.\n",
    "\n",
    "These skills are essential for working with AI-powered applications that combine large language models with domain-specific knowledge. Great work!\n",
    "\n",
    "Keep experimenting and exploring—there’s so much more you can build with these tools. 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
